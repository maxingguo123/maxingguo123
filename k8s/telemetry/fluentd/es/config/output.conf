# Enriches records with Kubernetes metadata
<match kubernetes.var.log.containers.canal**>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern /.*[INFO].*/
    tag ignore
  </rule>
</match>

<filter kubernetes.**>
  @type kubernetes_metadata
</filter>

<filter **>
  @type record_transformer
  <record>
    test TEST_NOTSET #subst test id here
    runid RUNID_NOTSET #subst run id here
    cluster-name {{$.Values.cluster}}
    subcluster-name {{$.curUser}}
  </record>
</filter>

<filter **>
  @type record_transformer
  enable_ruby
  auto_typecast
  <record>
    collection_time ${t=Time.now; t.to_i * (10 ** 9) + t.nsec}
  </record>
</filter>

<filter kubernetes.var.log.containers.**shc-chyenne**.log>
  @type record_transformer
  enable_ruby
  <record>
     #Strip ANSI colors. Reference: https://en.wikipedia.org/wiki/ANSI_escape_code
     #Similar to the following fluentd plugin
     #https://github.com/mattheworiordan/fluent-plugin-color-stripper
     log ${record["log"].gsub(/\033\[\d{1,2}(;\d{1,2}){0,2}[mGK]/, '')}
  </record>
</filter>

# Adds 2 new fields that can be used in data analysis to sort logs
# fluentd_start_time - start time epoch (nanoseconds) of the fluentd daemon.
# fluentd_counter - processed line counter. This counter resets to zero on fluentd restart.
<filter **>
  @type record_modifier
  prepare_value @global_counter = 0; t = Time.now; @fluentd_start_time = t.to_i * (10 ** 9) + t.nsec
  <record>
    fluentd_counter ${@global_counter+=1; @global_counter}
    fluentd_start_time ${@fluentd_start_time}
  </record>
</filter>

<filter **>
  @type prometheus
  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
</filter>

# Concatenate multi-line logs
<filter **>
  @type concat
  key message
  multiline_end_regexp /\n$/
  separator ""
</filter>


<match ignore>
  @type null
</match>


<filter **>
  @type elasticsearch_genid
  hash_id_key _hash
</filter>

<match **>
  @id elasticsearch
  @type elasticsearch
  @log_level info
  type_name fluentd
  include_tag_key true
  hosts {{range .Values.elasticsearch.hosts}}{{.host}}:{{.port}},{{end}}
  {{if .Values.elasticsearch.user}}user {{.Values.elasticsearch.user}}{{end}}
  {{if .Values.elasticsearch.password}}password {{.Values.elasticsearch.password}}{{end}}
  scheme {{.Values.elasticsearch.scheme}}
  id_key _hash
  remove_keys _hash
  reload_connections false
  reconnect_on_error true
  {{if eq .Values.elasticsearch.scheme "https"}}
  ssl_verify false
  ssl_version TLSv1_2
  {{end}}
 
  logstash_format true
  logstash_prefix {{.Values.esindex}}
  <buffer>
    @type file
    path /var/log/fluentd-buffers/kubernetes.system.buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 30s
    retry_timeout 1h
    retry_max_interval 120000
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>
