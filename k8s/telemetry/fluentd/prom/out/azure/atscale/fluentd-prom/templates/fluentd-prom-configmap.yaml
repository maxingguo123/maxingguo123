---
# Source: fluentd-prom/templates/fluentd-prom-configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: fluentd-prom-config-v0.1.6
  namespace: mgmt
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
data:

  containers.input.conf: |-
    <source>
      @id fluentd-containers.log
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/prom-containers.log.pos
      tag raw.kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>\S+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>
    
    <match raw.kubernetes.var.log.containers.**fluentd-prom**.log>
      @type null
    </match>
    
    <match raw.kubernetes.var.log.containers.**opcm-collector**.log>
      @type null
    </match>
    
    <match raw.kubernetes.var.log.containers.**kafka**.log>
      @type null
    </match>
    
    
    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>
      

  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      @type forward
    </source>
      

  monitoring.conf: |-
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @type prometheus
    </source>
    
    # input plugin that collects metrics for output plugin
    <source>
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
      

  output.conf: |-
    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>
    
    <filter kubernetes.**>
      @type record_modifier
      <record>
        cluster-name azure
      </record>
    </filter>
    
    <match kubernetes.var.log.containers.pts**>
      @type rewrite_tag_filter
      <rule>
        key log
        pattern /^[\s]*\"[^"]*\",(HIB|LIB),[\d\.]*/
        tag pts.result
      </rule>
    </match>
    
    <match kubernetes.var.log.containers.fluentd-es**>
      @type rewrite_tag_filter
      <rule>
         key log
         pattern /es_rejected_execution_exception/
         tag es.rejected
      </rule>
      <rule>
         key log
         pattern /json_parse_exception.*UTF-8/
         tag es.utf8
      </rule>
      <rule>
         key log
         pattern /failed.*Elasticsearch.*error/
         tag es.other
      </rule>
    </match>
    
    # <match kubernetes.var.log.containers.node-heartbeat**>
    <match kubernetes.**>
      @type rewrite_tag_filter
      <rule>
        key log
        pattern /^[\s]*Heartbeat: 0/
        tag heartbeat_0
      </rule>
    
      <rule>
        key log
        pattern /^[\s]*Heartbeat:/
        tag heartbeat
      </rule>
    
      <rule>
        key log
        #Sample - '# WARNING: test zstd time over 25% more than the expected time (2495.42 ms > 625 ms). Please check the test'
        pattern /more than the expected time/
        tag took_more_time
      </rule>
    
      <rule>
        key log
        #Sample - 'WARNING: test ipsec_aes128_docsis_hmac_sha1_avx took less than 80% of the expected time (196.232 ms < 240 ms). Please check the test'
        pattern /less than [\S]* of the expected time/
        tag took_less_time
      </rule>
    
      <rule>
        key log
        pattern /test-start-indicator/
        tag test_start
      </rule>
    
      <rule>
        key log
        pattern /test-end-indicator/
        tag test_end
      </rule>
    
      <rule>
        key log
        pattern /TEST PASSED/
        tag test_pass
      </rule>
    
      <rule>
        key log
        pattern /TEST FAILED/
        tag test_fail
      </rule>
    
      <rule>
        key log
        #Look for not ok. Word boundary anywhere
        # https://www.rexegg.com/regex-boundaries.html#wordboundary
        pattern /\bnot ok\b/
        tag notok
      </rule>
    
    #   <rule>
    #     key log
    #     #Look for ok. Word boundary anywhere
    #     pattern /\bok\b/
    #     tag ok
    #   </rule>
    
      <rule>
        key log
        #False positive for oom kill. Tag to ignore.
        #Sample - 'log_monitor.go:64] Finish parsing log monitor config file: {WatcherConfig:{Plugin:kmsg PluginConfig:map[] LogPath:/dev/kmsg Lookback:5m Delay:} BufferSize:100 Source:kernel-monitor DefaultConditions:[] Rules:[{Type:permanent Condition: Reason:Hardware Error Pattern:mce:.*} {Type:permanent Condition: Reason:Hardware Error Pattern:traps.*} {Type:permanent Condition: Reason:Hardware Error Pattern:runc:*} {Type:permanent Condition: Reason:Hardware Error Pattern:Code:*} {Type:temporary Condition: Reason:Hardware Error Pattern:oom-kill:.*'
        pattern /log_monitor.*oom-kill/
        tag false.oom_kill
      </rule>
    
       <rule>
        key log
        #Sample - 'oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=b0eb25f1d4ffa5dd44f41ae601c56f36b9ee6feccca5427e50255483eb857bb3,mems_allowed=0-1,oom_memcg=/kubepods/burstable/podf71f348b-6ac3-4559-ab9d-9c0726e6c6b6,task_memcg=/kubepods/burstable/podf71f348b-6ac3-4559-ab9d-9c0726e6c6b6/b0eb25f1d4ffa5dd44f41ae601c56f36b9ee6feccca5427e50255483eb857bb3,task=node-problem-de,pid=13973,uid=0'
        pattern /oom-kill/
        tag oom_kill
      </rule>
    
      <rule>
        key log
        #Sample - 'Mar 13 09:01:48 r15s09 kernel: Memory cgroup out of memory: Killed process 2979 (kube-rbac-proxy) total-vm:135868kB, anon-rss:29748kB, file-rss:0kB, shmem-rss:0kB, UID:65534 pgtables:168kB oom_score_adj:999'
        pattern /out of memory/i
        tag oom_kill
      </rule>
    
      <rule>
        key log
        #Sample - 'mcelog[1111]: Hardware event. This is not a software error.'
        pattern /mcelog.* Hardware event/
        tag mce
      </rule>
    
      <rule>
        key log
        #Sample - 'mcelog[1111]: Corrected error'
        pattern /mcelog.* Corrected error/
        tag corrected_mce
      </rule>
    
    
      <rule>
        key log
        #Sample - '[    3.654071] mce: [Hardware Error]: PROCESSOR 0:50654 TIME 1576079913 SOCKET 1 APIC 40 microcode 200005e'
        pattern /\[Hardware Error\]/
        tag hwerror
      </rule>
    
      <rule>
        key log
        #Sample - 'Dec 12 22:25:21 ip-192-168-0-92 kernel: mce: CPU0: Package temperature above threshold, cpu clock throttled (total events = 1000)'
        pattern /mce: .* temperature above threshold/
        tag thermal_mce
      </rule>
    
      <rule>
        key log
        pattern /ADDED.*NodeNotReady/
        tag nodenotready
      </rule>
    
    </match>
    
    <match kubernetes.**>
      @type null
    </match>
    
    <filter pts.result>
      @type parser
      key_name log
      reserve_data true
      reserve_time true
      <parse>
        @type regexp
        expression /^[\s]*\"(?<BENCHMARK>[^"]*)\",(?<TYPE>HIB|LIB),(?<RESULT>[\d]*.*)/
        types RESULT:float
      </parse>
    </filter>
    
    <filter nodenotready.**>
      @type parser
      key_name log
      reserve_data true
      reserve_time true
      <parse>
        @type regexp
        expression /"verb"[\s]*:[\s]*"ADDED".*[\s]*"involvedObject"[\s]*:[\s]*{[\s]*"kind"[\s]*:[\s]*"Node"[\s]*,[\s]*"name"[\s]*:[\s]*"(?<_NODE>[^"]*)"/
      </parse>
    </filter>
    
    <match false.**>
      @type null
    </match>
    
    <match nodenotready.**>
      @type copy
      <store>
        @type prometheus
        <labels>
          cluster $.cluster
          host ${_NODE}
          event ${tag}
          runid $.metadata.test.runid
          test $.metadata.test.name
        </labels>
        <metric>
          name fluentd_prom_node_status
          type counter
          desc Node status
        </metric>
      </store>
    </match>
    
    <match pts.result>
      @type copy
      <store>
        @type prometheus
        <labels>
          host $.kubernetes.host
          test $.test
          benchmark ${BENCHMARK}
          type ${TYPE}
        </labels>
        <metric>
          name fluentd_prom_pts_results
          type gauge
          desc Results from benchmark
          key RESULT
        </metric>
      </store>
    </match>
    
    <match metadata.**>
      @type null
    </match>
    
    <match **>
      @type copy
      <store>
        @type prometheus
        <labels>
          cluster $.cluster
          host $.kubernetes.host
          event ${tag}
          runid $.metadata.test.runid
          test $.metadata.test.name
        </labels>
        <metric>
          name fluentd_prom_node_status
          type counter
          desc Node status
        </metric>
      </store>
      <store>
      
        @id elasticsearch
        @type elasticsearch
        @log_level info
        type_name fluentd
        include_tag_key true
        hosts 172.26.0.191:9200,
        user elastic
        password EcN9fQY1s4516cF3HyR0w7r6
        scheme https
        reload_connections false
        reconnect_on_error true
        
        ssl_verify false
        ssl_version TLSv1_2
        
        logstash_format true
        logstash_prefix critical-qpool
        <buffer>
          @type file
          path /var/log/fluentd-prom-buffers/kubernetes.system.buffer
          flush_mode interval
          retry_type exponential_backoff
          flush_thread_count 4
          flush_interval 10s
          retry_forever
          retry_max_interval 32
          chunk_limit_size 1M
          queue_limit_length 32
          overflow_action block
        </buffer>
      
      </store>
    </match>
      

  system.conf: |-
    <system>
      @log_level fatal
      root_dir /tmp/fluentd-buffers/
      rpc_endpoint 0.0.0.0:24444
    </system>
      


  metadata.conf: |-
    <source>
      @id test_meta_file
      @type tail
      path /var/log/atscale/test-metadata.ndjson
      format json
      read_lines_limit 1
      read_from_head true
      tag metadata.test
    </source>
    
    # Add test metadata
    # __dummy*__ are temporary keys that will be discarded later
    # Instead of assigning values to these keys, we have ruby snippet that will update record with metadata
    
    <filter **>
      @type record_modifier
      prepare_value $test_metadata = Hash.new
    
    # Cache metadata in global variable
      <record>
        __dummy1__ ${(tag == 'metadata.test') ? ($test_metadata=record.clone) : nil}
      </record>
    
    # If the log doesn't have a field called "metadata", add an empty hash
      <record>
        __dummy2__ ${!tag.start_with?('metadata.') && !record.key?("metadata") ? record["metadata"]=Hash.new : nil}
      </record>
    
    # Copy cached metadata to log
      <record>
        __dummy3__ ${ (!tag.start_with?('metadata.') && !$test_metadata.nil?) ? record["metadata"]["test"]=$test_metadata : nil}
      </record>
    
      remove_keys __dummy1__, __dummy2__, __dummy3__
    
    </filter>
    
    <source>
      @id node_meta_file
      @type tail
      path /var/log/atscale/node-metadata.ndjson
      format json
      read_lines_limit 1
      read_from_head true
      tag metadata.node
    </source>
    
    # Add node metadata
    <filter **>
      @type record_modifier
      prepare_value $node_metadata = Hash.new
    
    
    # Cache metadata in global variable
      <record>
        __dummy1__ ${(tag == 'metadata.node') ? ($node_metadata=record.clone) : nil}
      </record>
    
    # If the log doesn't have a field called "metadata", add an empty hash
      <record>
        __dummy2__ ${!tag.start_with?('metadata.') && !record.key?("metadata") ? record["metadata"]=Hash.new : nil}
      </record>
    
    # Copy cached metadata to log
      <record>
        __dummy3__ ${(!tag.start_with?('metadata.') && !$node_metadata.nil?) ? record["metadata"]["node"]=$node_metadata : nil}
      </record>
    
      remove_keys __dummy1__, __dummy2__, __dummy3__
    
    </filter>
    
    <source>
      @id cscope_metadata_file
      @type tail
      path /var/log/atscale/cscope-metadata.ndjson
      format json
      read_lines_limit 1
      read_from_head true
      tag metadata.cscope
    </source>
    
    # Add clusterscope metadata
    <filter **>
      @type record_modifier
      prepare_value $cscope_metadata = Hash.new
    
    
    # Cache metadata in global variable
      <record>
        __dummy1__ ${(tag == 'metadata.cscope') ? ($cscope_metadata=record.clone) : nil}
      </record>
    
    # If the log doesn't have a field called "metadata", add an empty hash
      <record>
        __dummy2__ ${!tag.start_with?('metadata.') && !record.key?("metadata") ? record["metadata"]=Hash.new : nil}
      </record>
    
    # Copy cached metadata to log
      <record>
        __dummy3__ ${(!tag.start_with?('metadata.') && !$cscope_metadata.nil?) ? record["metadata"]["cscope"]=$cscope_metadata : nil}
      </record>
    
      remove_keys __dummy1__, __dummy2__, __dummy3__
    
    </filter>
