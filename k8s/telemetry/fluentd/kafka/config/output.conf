# Enriches records with Kubernetes metadata
<match kubernetes.var.log.containers.prometheus-adapter**>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern /unable to fetch .* metrics for/
    tag ignore
  </rule>
  <rule>
    key log
    pattern /.+/
    tag prometheus-adapter.${tag}
  </rule>
</match>

<match kubernetes.var.log.containers.canal**>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern /.*[INFO].*/
    tag ignore
  </rule>
  <rule>
    key log
    pattern /.+/
    tag canal.${tag}
  </rule>
</match>


<filter systemd.**>
  @type systemd_entry
  field_map {"MESSAGE": "log"}
  field_map_strict false
  fields_lowercase true
  fields_strip_underscores true
</filter>

<filter kubernetes.**>
  @type kubernetes_metadata
</filter>

<filter **>
  @type record_modifier
  <record>
    cluster-name {{$.Values.cluster}}
  </record>
</filter>

<filter kubernetes.var.log.containers.**shc-chyenne**.log>
  @type record_modifier
  <record>
     #Strip ANSI colors. Reference: https://en.wikipedia.org/wiki/ANSI_escape_code
     #Similar to the following fluentd plugin
     #https://github.com/mattheworiordan/fluent-plugin-color-stripper
     log ${record["log"].gsub(/\033\[\d{1,2}(;\d{1,2}){0,2}[mGK]/, '')}
  </record>
</filter>

# Adds 2 new fields that can be used in data analysis to sort logs
# fluentd_start_time - start time epoch (nanoseconds) of the fluentd daemon.
# fluentd_counter - processed line counter. This counter resets to zero on fluentd restart.
<filter **>
  @type record_modifier
  prepare_value @global_counter = 0; t = Time.now; @fluentd_start_time = t.to_i * (10 ** 9) + t.nsec
  <record>
    fluentd_counter ${@global_counter+=1; @global_counter}
    fluentd_start_time ${@fluentd_start_time}
  </record>
</filter>

<filter **>
  @type prometheus
  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      hostname ${hostname}
    </labels>
  </metric>
</filter>

<match kubernetes.var.log.containers.**ptu-monitor**>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern /^[\s]*[\d]*[\s]*CPU\d/
    tag ptu.measurements
  </rule>
  <rule>
    key log
    pattern /.+/
    tag ptu.${tag}
  </rule>
</match>

<filter ptu.measurements>
  @type parser
  key_name log
  reserve_data true
  reserve_time true
  inject_key_prefix ptu.
  <parse>
    @type regexp
    expression /^[\s]*(?<index>[\d]*)[\s]*(?<cpu>CPU\d)[\s-]*(?<cfreq>[\d]*)[\s]*(?<ufreq>[\d]*)[\s]*(?<util>[\d\.]*)[\s]*(?<ipc>[\d\.]*)[\s]*(?<c0>[\d\.]*)[\s]*(?<c1>[\d\.]*)[\s]*(?<c6>[\d\.]*)[\s]*(?<pc2>[\d\.]*)[\s]*(?<pc6>[\d\.]*)[\s-]*(?<temp>[\d]*)[\s-]*(?<dts>[\d]*)[\s]*(?<power>[\d\.]*)[\s]*(?<volt>[\d\.]*)[\s]*(?<uvolt>[\d\.]*)[\s]*(?<tstat>[\dx]*)[\s]*(?<tlog>[\dx]*)[\s]*(?<tl>[\d]*)[\s]*(?<tmargin>[\d\.]*)/
    types index:integer,cfreq:integer,ufreq:integer,util:float,ipc:float,c0:float,c1:float,c6:float,pc2:float,pc6:float,temp:integer,dts:integer,power:float,volt:float,uvolt:float,tl:integer,tlmargin:float
  </parse>
</filter>

<match kubernetes.var.log.containers.cluster-msr**>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern /^MSR\s*[\d|[a-zA-z]]*\s*has\svalues\s*\{\'/
    tag msr.field
  </rule>
</match>

<filter msr.field>
  @type parser
  key_name log
  reserve_data true
  reserve_time true
  inject_key_prefix msr.
  <parse>
    @type regexp
    expression /^MSR\s*(?<key>[\d|[a-zA-z]]*)\s*has\svalues\s*\{\'(?<value>[\d|[a-zA-z]]*)\s*/
  </parse>
</filter>

<filter msr.field>
  @type record_transformer
  enable_ruby
  <record>
    msr.decimal ${record["msr.value"].to_i(16)}
  </record>
</filter>

<match kubernetes.var.log.containers.hp-linpack**>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern /^[\s]*[\d\.\s\:]**hp-linpack-[\d\w]*\[[\d\,]*\]/
    tag hplinpack.measurements
  </rule>
  <rule>
    key log
    pattern /^[\s]*Peak Performance[\s]*\=[\s]*[\d\.]*[\s]*GFlops/
    tag hplinpack.result
  </rule>
  <rule>
    key log
    pattern /.+/
    tag hplinpack.${tag}
  </rule>
</match>

<filter hplinpack.measurements>
  @type parser
  key_name log
  reserve_data true
  reserve_time true
  inject_key_prefix hplinpack.
  <parse>
    @type regexp
    expression /^[\s]*(?<Frac>[\d\.]*)[\s]*(?<n>[\d\.]*)[\s]*(?<PFact>[\d\.]*)[\s]*(?<Bcast>[\d\.]*)[\s]*(?<Swap>[\d\.]*)[\s]*(?<Update>[\d\.]*)[\s]*(?<PPerf>[\d\.]*)[\s]*(?<BcstBW>[\d\.]*)[\s]*(?<SwapBW>[\d\.]*)[\s]*(?<CPU>[\d\.]*)[\s]*(?<Kernel>[\d\.]*)[\s]*(?<Total>[\d\.]*)[\s]*(?<Powr>[\d\.]*)[\s]*(?<Dpwr>[\d\.]*)[\s]*(?<Tmp>[\d\.]*)[\s]*(?<CFreq>[\d\.]*)[\s]*(?<UFreq>[\d\.]*)[\s]*(?<Time>[\d\:\.]*)[\s]*(?<WhoamI>[\w\-\[\,\]]*)/
    types Frac:float,n:integer,PFact:float,Bcast:float,Swap:float,Update:float,PPerf:float,BcstBW:float,SwapBW:float,CPU:float,Kernel:float,Total:float,Powr:float,Dpwr:float,Tmp:float,CFreq:float,UFreq:float 
  </parse>
</filter>

<filter hplinpack.result>
  @type parser
  key_name log
  reserve_data true
  reserve_time true
  inject_key_prefix benchmark.
  <parse>
    @type regexp
    expression /^[\s]*Peak Performance[\s]*\=[\s]*(?<result>[\d\.]*)[\s]*GFlops/
  </parse>
</filter>

<match kubernetes.var.log.containers.pts**>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern /^[\s]*\"[^"]*\",(HIB|LIB),[\d\.]*/
    tag pts.result
  </rule>
  <rule>
    key log
    pattern /.+/
    tag pts.${tag}
  </rule>
</match>

<filter pts.result>
  @type parser
  key_name log
  reserve_data true
  reserve_time true
  inject_key_prefix benchmark.
  <parse>
    @type regexp
    expression /^[\s]*\"(?<name>[^"]*)\",(?<type>HIB|LIB),(?<result>[\d]*.*)/
    types result:float
  </parse>
</filter>

# Concatenate multi-line logs
<filter multiline.**>
  @type concat
  key message
  multiline_end_regexp /\n$/
  separator ""
</filter>

<match ignore>
  @type null
</match>

# Remove metadata from sol data.
# TODO: Revisit metadata on telemetry nodes
<filter sol.logs>
  @type record_modifier
  remove_keys metadata
  <record>
    kafkatopic sol
  </record>
</filter>

# goal: send specific log line to two different kafka topics: atscale and cluster-pnp
# tag specific log line
<match kubernetes.var.log.containers.paiv-pnp**>
  @type rewrite_tag_filter
  <rule>
    key log
    pattern /Flag_Cluster_PnP_SPIV_SO.*/
    tag paiv-pnp
  </rule>
  <rule>
    key log
    pattern /.+/
    tag atscale
  </rule>
</match>
# duplicate our log line, tag one for each kafka topic
<match paiv-pnp>
  @type copy
  <store>
    @type rewrite_tag_filter
    <rule>
      key log
      pattern /.+/
      tag clusterpnp
    </rule>
  </store>
  <store>
    @type rewrite_tag_filter
    <rule>
      key log
      pattern /.+/
      tag atscale
    </rule>
  </store>
</match>
# modify kafkatopic for each copy of record
<filter clusterpnp>
  @type record_modifier
  <record>
    kafkatopic cluster-pnp
  </record>
</filter>
<filter atscale>
  @type record_modifier
  <record>
    kafkatopic atscale
  </record>
</filter>

# throttle plugin settings
#<filter kubernetes.** >
#  @type throttle
#  group_key kubernetes.host,kubernetes.pod_name
#  group_bucket_period_s   60
#  group_bucket_limit    6000
#  group_reset_rate_s     100
#  group_drop_logs false
#</filter>
#<filter systemd.**>
#  @type throttle
#  group_key hostname,syslog_identifier
#  group_bucket_period_s   60
#  group_bucket_limit    6000
#  group_reset_rate_s     100
#  group_drop_logs false
#</filter>


<match **>
  @type rdkafka2
  @log_level info

  brokers {{range $.Values.kafka}}{{.host}}:{{.port}},{{end}}

  # Behavior of fluentd : If the record contains a field with name matching the 'topic_key' it will use the value of the field to decide where to send the logs
  # If not it will use the topic specified in the 'default_topic' setting.
  # Important: This will only work if the buffer chunk key is same as topic_key.
  topic_key kafkatopic
  default_topic {{$.Values.topic}}

  # Exclude topic_key. Don't need that on record
  exclude_topic_key true 

  # ruby-kafka configuration
  max_send_limit_bytes 10485760
  # kafka_agg_max_bytes 10485760
  # kafka_agg_max_messages 5000
  #get_kafka_client_log true
  #compression_codec snappy
  #idempotent true
  rdkafka_options {
    "enable.idempotence" : true,
    "compression.codec": "snappy"
  }

  <format>
    @type json
  </format>

  <inject>
    tag_key tag
    time_key timestamp_ns
    time_type string
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </inject>
  # See fluentd document for buffer related parameters: https://docs.fluentd.org/v/1.0/configuration/buffer-section
  # Buffer chunk key should be same with topic_key. If value is not found in the record, default_topic is used.
  <buffer tag, kafkatopic>
    @type file
    path /var/log/fluentd-buffers/fluentd-kafka.buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 3
    flush_interval 30s
    retry_forever true
    retry_max_interval 120000
    chunk_limit_size 10M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>

