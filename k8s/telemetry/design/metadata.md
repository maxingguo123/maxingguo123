## Objective
* Provides ability to add test metadata to logs that indicate the test name and run id.
* Provides ability to add node metadata. Basically scrape all node labels and attach that information.

## Sample metadata:
This will be part of the json logs:

```json
"metadata": {
      "test": {
        "runid": "run-qylo5a",
        "name": "dol-unified"
      },
      "node": {
        "nodeowner": "default",
        "pull-image": "true",
        "katacontainers.io/kata-runtime": "true",
        "localadmin.io-kata-node": "true",
        "unifiednode": "true"
      }
    }
```

## Highlights:
* With the current design we do not have to reload fluentd configuration to get changes in metadata. Reloading fluentd was causing intermittent issues corrupting the buffer before. This wonâ€™t be a problem anymore.
* The metadata fields are dynamically loaded at run time and provides flexibility to add or remove new metadata fields at run time.
* This solution works for any test workload as long as the metadata file is present in the expected path. It should readily usable for tests run on all the tenants such as piv, ive etc.
* This simplifies fluentd deployment in multitenant setup. The previous model required separate daemonsets for each tenant running in different k8s namespaces. The new model requires just one daemonset.

## How it works:
* This solution works by fluentd tailing metadata in the following files on the host : /var/log/atscale/test-metadata.ndjson /var/log/atscale/node-metadata.json. These contain test metadata and node metadata respectively.
* Node metadata is generated and managed by the node tracker daemonset. It will automatically pick up any Kubernetes labels on the node.
* Test metadata is generated by test runner. All tests run using Jenkins test runner should already have it.

## Structure of test-metadata.ndjson
* Each line should be a self contained json containing all of the metadata.
* fluentd is setup to tail and always pick the most recent line for metadata.
* A sample test metatajson would look like this:
```ndjson
  {"name": "sandstone-unified", "runid": "run-yqx30m"}     
  {"name": "NOTSET", "runid": "NOTSET"}                    
  {"name": "jasper-vt-combo", "runid": "run-7xo4i4"}       
  {"name": "sandstone-chaos", "runid": "run-xabqcw"}       
  {"name": "NOTSET", "runid": "NOTSET"}                    
  {"name": "sandstone-chaos", "runid": "run-dtyl97"}       
  {"name": "NOTSET", "runid": "NOTSET"}                    
  {"name": "NOTSET", "runid": "NOTSET"}                    
  {"name": "sandstone-chaos", "runid": "run-h31tlx"}       
  {"name": "NOTSET", "runid": "NOTSET"}                    
  {"name": "sandstone-chaos", "runid": "run-nvjn3e"}       
  {"name": "NOTSET", "runid": "NOTSET"}                    
  {"name": "dol-unified", "runid": "run-qylo5a"}           
```
* At the minimum we recommend having 'name' and 'runid' fields. We can have any number fields to this json.
| NOTE: Be mindful that each field added here will be copied to every single log file that is generated. Use best judgment in picking the required fields. 
* It is also recommended to mark the testname and runid as 'NOTSET' to indicate the test had completed.
* Please refer to the following files for reference implemntation:
  - Creating metadata at the beginning of test run: https://gitlab.devtools.intel.com/sandstone/cluster-infra/-/blob/master/k8s/sandstone_run_specific.yaml#L69
  - Resetting metadata at the end of test run: https://gitlab.devtools.intel.com/sandstone/cluster-infra/-/blob/master/k8s/sandstone_run_completed.yaml#L23
